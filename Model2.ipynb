{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4382e23e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## üì¶ 1. Install Required Libraries\n",
    "# !pip install datasets transformers torch torchvision pandas pillow scikit-learn tqdm\n",
    "\n",
    "## üìö 2. Import Libraries\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    ViTModel, ViTImageProcessor,\n",
    "    AutoTokenizer, AutoModel,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "# Computer Vision\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Device Selection\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üîß Device Used: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Available Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "## üìä 3. Load and Analyze Data from Local Files\n",
    "def load_local_vqa_rad_data(excel_path, image_folder_path, test_split=0.2, val_split=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Load VQA-RAD data from local Excel file and image folder\n",
    "    \"\"\"\n",
    "    print(\"‚¨áÔ∏è Loading VQA-RAD data from local files...\\n\")\n",
    "    \n",
    "    # Check if files exist\n",
    "    if not os.path.exists(excel_path):\n",
    "        raise FileNotFoundError(f\"‚ùå Excel file not found: {excel_path}\")\n",
    "    \n",
    "    # Read Excel file\n",
    "    df = pd.read_excel(excel_path)\n",
    "    print(f\"‚úÖ Excel file loaded successfully: {len(df)} rows\")\n",
    "    \n",
    "    # Show first rows to understand structure\n",
    "    print(\"\\nüìã First rows of data:\")\n",
    "    print(df[['QUESTION', 'ANSWER', 'IMAGEID']].head())\n",
    "    \n",
    "    # Map column names\n",
    "    if 'QUESTION' in df.columns and 'question' not in df.columns:\n",
    "        df['question'] = df['QUESTION']\n",
    "    \n",
    "    if 'ANSWER' in df.columns and 'answer' not in df.columns:\n",
    "        df['answer'] = df['ANSWER']\n",
    "    \n",
    "    if 'IMAGEID' in df.columns and 'image_name' not in df.columns:\n",
    "        df['image_name'] = df['IMAGEID']\n",
    "    \n",
    "    # Prepare Dataset\n",
    "    dataset = {'train': [], 'val': [], 'test': []}\n",
    "    train_count = 0\n",
    "    val_count = 0\n",
    "    test_count = 0\n",
    "    missing_images = 0\n",
    "    found_images = 0\n",
    "    \n",
    "    # Shuffle data\n",
    "    df = df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    # Get list of all image files in folder\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG', '.bmp', '.BMP', '.tiff', '.TIFF']\n",
    "    all_image_files = []\n",
    "    for ext in image_extensions:\n",
    "        all_image_files.extend(glob.glob(os.path.join(image_folder_path, f\"*{ext}\")))\n",
    "    \n",
    "    # Create dictionary to map filenames without extension to full paths\n",
    "    image_files_dict = {}\n",
    "    for img_path in all_image_files:\n",
    "        filename = os.path.basename(img_path)\n",
    "        # Remove extension for matching\n",
    "        filename_no_ext = os.path.splitext(filename)[0].lower()\n",
    "        image_files_dict[filename_no_ext] = img_path\n",
    "    \n",
    "    print(f\"\\nüìÅ Found {len(all_image_files)} images in folder: {image_folder_path}\")\n",
    "    print(f\"   Sample image names: {[os.path.basename(f) for f in all_image_files[:5]]}\")\n",
    "    \n",
    "    # Process each row\n",
    "    valid_rows = []\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            # Get image URL from IMAGEID\n",
    "            image_url = str(row['IMAGEID'])\n",
    "            \n",
    "            # Extract image name (last part of URL)\n",
    "            image_name = os.path.basename(image_url)\n",
    "            image_name_no_ext = os.path.splitext(image_name)[0].lower()\n",
    "            \n",
    "            # Search for local image\n",
    "            image_path = None\n",
    "            \n",
    "            # 1. Try direct matching\n",
    "            if image_name_no_ext in image_files_dict:\n",
    "                image_path = image_files_dict[image_name_no_ext]\n",
    "            \n",
    "            # 2. If not found, try partial matching\n",
    "            if not image_path:\n",
    "                for img_key, img_path in image_files_dict.items():\n",
    "                    if image_name_no_ext in img_key or img_key in image_name_no_ext:\n",
    "                        image_path = img_path\n",
    "                        break\n",
    "            \n",
    "            if image_path:\n",
    "                found_images += 1\n",
    "            else:\n",
    "                missing_images += 1\n",
    "                print(f\"‚ö†Ô∏è Could not find image for {image_name_no_ext} (Row {idx})\")\n",
    "                continue\n",
    "            \n",
    "            # Store example\n",
    "            example = {\n",
    "                'image': image_path,  # Will be the path\n",
    "                'image_path': image_path,\n",
    "                'question': str(row['question']) if 'question' in row else str(row['QUESTION']),\n",
    "                'answer': str(row['answer']) if 'answer' in row else str(row['ANSWER']),\n",
    "                'question_type': str(row.get('Q_TYPE', 'N/A')),\n",
    "                'image_name': os.path.basename(image_path),\n",
    "                'original_image_url': image_url\n",
    "            }\n",
    "            \n",
    "            # Add additional metadata\n",
    "            if 'QID_unique' in row:\n",
    "                example['qid_unique'] = str(row['QID_unique'])\n",
    "            if 'QID_linked' in row:\n",
    "                example['qid_linked'] = str(row['QID_linked'])\n",
    "            \n",
    "            valid_rows.append(example)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error processing row {idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Split data into train, validation, and test\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    train_data, temp_data = train_test_split(\n",
    "        valid_rows, test_size=(test_split + val_split), random_state=random_state\n",
    "    )\n",
    "    \n",
    "    val_data, test_data = train_test_split(\n",
    "        temp_data, test_size=test_split/(test_split + val_split), random_state=random_state\n",
    "    )\n",
    "    \n",
    "    dataset['train'] = train_data\n",
    "    dataset['val'] = val_data\n",
    "    dataset['test'] = test_data\n",
    "    \n",
    "    train_count = len(train_data)\n",
    "    val_count = len(val_data)\n",
    "    test_count = len(test_data)\n",
    "    \n",
    "    print(f\"\\nüìä Dataset Summary:\")\n",
    "    print(f\"   - Training examples: {train_count}\")\n",
    "    print(f\"   - Validation examples: {val_count}\")\n",
    "    print(f\"   - Testing examples: {test_count}\")\n",
    "    print(f\"   - Images found: {found_images}\")\n",
    "    print(f\"   - Missing images: {missing_images}\")\n",
    "    \n",
    "    if missing_images > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è Warning: {missing_images} images not found locally.\")\n",
    "        print(\"   Script looks for images with names like 'synpic54610' or just '54610'\")\n",
    "        print(\"   Make sure image filenames match synpic numbers in Excel file.\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Define paths - update to match your local paths\n",
    "EXCEL_PATH = \"/content/VQA_RAD Dataset Public.xlsx\"  # Path to your Excel file\n",
    "IMAGE_FOLDER_PATH = \"/content/VQA_RAD Image Folder\"  # Path to your image folder\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    dataset = load_local_vqa_rad_data(\n",
    "        excel_path=EXCEL_PATH,\n",
    "        image_folder_path=IMAGE_FOLDER_PATH,\n",
    "        test_split=0.2,\n",
    "        val_split=0.1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Data loaded successfully!\")\n",
    "    print(f\"üìä Dataset Info:\")\n",
    "    print(f\"   - Training examples count: {len(dataset['train'])}\")\n",
    "    print(f\"   - Validation examples count: {len(dataset['val'])}\")\n",
    "    print(f\"   - Testing examples count: {len(dataset['test'])}\")\n",
    "    print(f\"   - Total examples: {sum(len(dataset[split]) for split in dataset)}\")\n",
    "\n",
    "    # Show example\n",
    "    print(\"\\nüìù Example from data:\")\n",
    "    example = dataset['train'][0]\n",
    "    print(f\"   Question: {example.get('question', 'N/A')}\")\n",
    "    print(f\"   Answer: {example.get('answer', 'N/A')}\")\n",
    "    print(f\"   Question type: {example.get('question_type', 'N/A')}\")\n",
    "    print(f\"   Image path: {example.get('image_path', 'N/A')}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Loading error: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "## üîç 4. Analyze Answer Distribution\n",
    "# Collect all answers\n",
    "all_answers = []\n",
    "for split in ['train', 'val', 'test']:\n",
    "    for example in dataset[split]:\n",
    "        answer = example.get('answer', '')\n",
    "        if answer:\n",
    "            all_answers.append(str(answer).lower().strip())\n",
    "\n",
    "# Analyze distribution\n",
    "answer_counts = Counter(all_answers)\n",
    "print(f\"\\nüìä Answer Statistics:\")\n",
    "print(f\"   - Unique answers count: {len(answer_counts)}\")\n",
    "print(f\"   - Top 10 most common answers:\")\n",
    "for answer, count in answer_counts.most_common(10):\n",
    "    print(f\"      '{answer}': {count} times\")\n",
    "\n",
    "# Select most common answers (TOP_K)\n",
    "TOP_K_ANSWERS = 500  # Can be adjusted as needed\n",
    "top_answers = [ans for ans, _ in answer_counts.most_common(TOP_K_ANSWERS)]\n",
    "print(f\"\\n‚úÖ We will use the top {TOP_K_ANSWERS} most common answers\")\n",
    "\n",
    "## üèóÔ∏è 5. Build Advanced Model\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Cross Attention for integrating visual and textual features\"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // num_heads\n",
    "\n",
    "        assert self.head_dim * num_heads == dim, \"dim must be divisible by num_heads\"\n",
    "\n",
    "        self.query = nn.Linear(dim, dim)\n",
    "        self.key = nn.Linear(dim, dim)\n",
    "        self.value = nn.Linear(dim, dim)\n",
    "        self.out = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # Linear projections in batch from dim => num_heads x head_dim\n",
    "        Q = self.query(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.key(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.value(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        # Apply attention to values\n",
    "        context = torch.matmul(attention, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.dim)\n",
    "\n",
    "        return self.out(context)\n",
    "\n",
    "\n",
    "class AdvancedFusionModule(nn.Module):\n",
    "    \"\"\"Advanced fusion module using Cross Attention and Gating Mechanism\"\"\"\n",
    "\n",
    "    def __init__(self, vision_dim=768, text_dim=768, hidden_dim=512, num_heads=8, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Projection layers\n",
    "        self.vision_proj = nn.Linear(vision_dim, hidden_dim)\n",
    "        self.text_proj = nn.Linear(text_dim, hidden_dim)\n",
    "\n",
    "        # Cross Attention: Text attends to Vision\n",
    "        self.text_to_vision_attention = MultiHeadCrossAttention(hidden_dim, num_heads, dropout)\n",
    "\n",
    "        # Cross Attention: Vision attends to Text\n",
    "        self.vision_to_text_attention = MultiHeadCrossAttention(hidden_dim, num_heads, dropout)\n",
    "\n",
    "        # Gating mechanism\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Layer Normalization\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm3 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, vision_features, text_features):\n",
    "        # Project to same dimension\n",
    "        vision = self.vision_proj(vision_features)  # [batch, seq_v, hidden]\n",
    "        text = self.text_proj(text_features)        # [batch, seq_t, hidden]\n",
    "\n",
    "        # Cross Attention\n",
    "        text_attended = self.text_to_vision_attention(text, vision, vision)\n",
    "        text = self.norm1(text + self.dropout(text_attended))\n",
    "\n",
    "        vision_attended = self.vision_to_text_attention(vision, text, text)\n",
    "        vision = self.norm2(vision + self.dropout(vision_attended))\n",
    "\n",
    "        # Pooling: mean over sequence dimension\n",
    "        vision_pooled = vision.mean(dim=1)  # [batch, hidden]\n",
    "        text_pooled = text.mean(dim=1)      # [batch, hidden]\n",
    "\n",
    "        # Gating mechanism for adaptive fusion\n",
    "        concat_features = torch.cat([vision_pooled, text_pooled], dim=-1)\n",
    "        gate = self.gate(concat_features)\n",
    "\n",
    "        # Gated fusion\n",
    "        fused = gate * vision_pooled + (1 - gate) * text_pooled\n",
    "        fused = self.norm3(fused)\n",
    "\n",
    "        # Feed-forward\n",
    "        fused = fused + self.ffn(fused)\n",
    "\n",
    "        return fused\n",
    "\n",
    "\n",
    "class VQAModel(nn.Module):\n",
    "    \"\"\"Main Model: ViT + BioBERT + Advanced Fusion\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, hidden_dim=512, num_heads=8, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Vision Encoder: ViT\n",
    "        self.vision_encoder = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        self.vision_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        vision_dim = self.vision_encoder.config.hidden_size  # 768\n",
    "\n",
    "        # Text Encoder: BioBERT\n",
    "        self.text_encoder = AutoModel.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
    "        text_dim = self.text_encoder.config.hidden_size  # 768\n",
    "\n",
    "        # Fusion Module\n",
    "        self.fusion = AdvancedFusionModule(\n",
    "            vision_dim=vision_dim,\n",
    "            text_dim=text_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # Classification Head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        # Extract vision features\n",
    "        vision_outputs = self.vision_encoder(pixel_values=images)\n",
    "        vision_features = vision_outputs.last_hidden_state  # [batch, 197, 768]\n",
    "\n",
    "        # Extract text features\n",
    "        text_outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_features = text_outputs.last_hidden_state  # [batch, seq_len, 768]\n",
    "\n",
    "        # Fuse features\n",
    "        fused_features = self.fusion(vision_features, text_features)\n",
    "\n",
    "        # Classify\n",
    "        logits = self.classifier(fused_features)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "print(\"\\n‚úÖ Model built successfully!\")\n",
    "print(\"\\nüìê Model Components:\")\n",
    "print(\"   1. Vision Transformer (ViT) - google/vit-base-patch16-224-in21k\")\n",
    "print(\"   2. BioBERT - dmis-lab/biobert-base-cased-v1.2\")\n",
    "print(\"   3. Multi-Head Cross Attention Fusion\")\n",
    "print(\"   4. Deep Classification Head\")\n",
    "\n",
    "## üì¶ 6. Prepare Dataset and DataLoader\n",
    "class VQARadDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for VQA-RAD data\"\"\"\n",
    "\n",
    "    def __init__(self, data, vision_processor, tokenizer, label_encoder, split='train', max_length=128):\n",
    "        self.data = data\n",
    "        self.vision_processor = vision_processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_encoder = label_encoder\n",
    "        self.max_length = max_length\n",
    "        self.split = split\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.data[idx]\n",
    "\n",
    "        # Process image - load image from path\n",
    "        try:\n",
    "            image = Image.open(example['image']).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not load image {example['image']}: {e}\")\n",
    "            # Create white image as fallback\n",
    "            image = Image.new('RGB', (224, 224), color='white')\n",
    "        \n",
    "        # Process image using Vision Processor\n",
    "        image_inputs = self.vision_processor(images=image, return_tensors='pt')\n",
    "        pixel_values = image_inputs['pixel_values'].squeeze(0)\n",
    "\n",
    "        # Process question\n",
    "        question = str(example.get('question', ''))\n",
    "        text_inputs = self.tokenizer(\n",
    "            question,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Process answer\n",
    "        answer = str(example.get('answer', '')).lower().strip()\n",
    "\n",
    "        # Encode label\n",
    "        if answer in self.label_encoder.classes_:\n",
    "            label = self.label_encoder.transform([answer])[0]\n",
    "        else:\n",
    "            label = -1  # Unknown answer\n",
    "\n",
    "        return {\n",
    "            'pixel_values': pixel_values,  # already tensor\n",
    "            'input_ids': text_inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': text_inputs['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long),\n",
    "            'answer_text': answer,\n",
    "            'question_text': question,\n",
    "            'image_path': example.get('image_path', '')\n",
    "        }\n",
    "\n",
    "# Custom function to collate data in batch\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"Custom function to collate data in batch\"\"\"\n",
    "    # Aggregate data by keys\n",
    "    collated_batch = {}\n",
    "    \n",
    "    # Collect all keys present in batch\n",
    "    keys = batch[0].keys()\n",
    "    \n",
    "    for key in keys:\n",
    "        if key in ['answer_text', 'question_text', 'image_path']:\n",
    "            # For text fields, collect in list\n",
    "            collated_batch[key] = [item[key] for item in batch]\n",
    "        else:\n",
    "            # For other fields, aggregate into tensor\n",
    "            collated_batch[key] = torch.stack([item[key] for item in batch])\n",
    "    \n",
    "    return collated_batch\n",
    "\n",
    "# Prepare Label Encoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(top_answers)\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"\\nüìä Number of Classes: {num_classes}\")\n",
    "\n",
    "# Initialize model\n",
    "model = VQAModel(num_classes=num_classes, hidden_dim=512, num_heads=8, dropout=0.2)\n",
    "model = model.to(device)\n",
    "\n",
    "# Prepare Datasets\n",
    "train_dataset = VQARadDataset(\n",
    "    dataset['train'],\n",
    "    model.vision_processor,\n",
    "    model.tokenizer,\n",
    "    label_encoder,\n",
    "    split='train'\n",
    ")\n",
    "\n",
    "val_dataset = VQARadDataset(\n",
    "    dataset['val'],\n",
    "    model.vision_processor,\n",
    "    model.tokenizer,\n",
    "    label_encoder,\n",
    "    split='val'\n",
    ")\n",
    "\n",
    "test_dataset = VQARadDataset(\n",
    "    dataset['test'],\n",
    "    model.vision_processor,\n",
    "    model.tokenizer,\n",
    "    label_encoder,\n",
    "    split='test'\n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "BATCH_SIZE = 8  # Can be adjusted based on available memory\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Use 0 to avoid multiprocessing issues\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=custom_collate_fn  # Add custom collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,  # Use 0 to avoid multiprocessing issues\n",
    "    pin_memory=True,\n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,  # Use 0 to avoid multiprocessing issues\n",
    "    pin_memory=True,\n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Datasets and DataLoaders prepared\")\n",
    "print(f\"   - Training batch size: {BATCH_SIZE}\")\n",
    "print(f\"   - Training batches count: {len(train_loader)}\")\n",
    "print(f\"   - Validation batches count: {len(val_loader)}\")\n",
    "print(f\"   - Testing batches count: {len(test_loader)}\")\n",
    "\n",
    "## üéØ 7. Training Setup\n",
    "# Training parameters\n",
    "NUM_EPOCHS = 15\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_STEPS = 100\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "num_training_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=WARMUP_STEPS,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Loss function with label smoothing\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1, label_smoothing=0.1)\n",
    "\n",
    "print(\"\\n‚úÖ Training components prepared\")\n",
    "print(f\"   - Number of Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"   - Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"   - Warmup Steps: {WARMUP_STEPS}\")\n",
    "print(f\"   - Total training steps: {num_training_steps}\")\n",
    "\n",
    "## üöÄ 8. Training and Evaluation Functions\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, criterion, device, epoch):\n",
    "    \"\"\"Train one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1} - Training\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        # Move to device\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Filter out unknown labels\n",
    "        valid_mask = labels != -1\n",
    "        if not valid_mask.any():\n",
    "            continue\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(pixel_values, input_ids, attention_mask)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(logits[valid_mask], labels[valid_mask])\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predictions = torch.argmax(logits[valid_mask], dim=-1)\n",
    "        correct += (predictions == labels[valid_mask]).sum().item()\n",
    "        total += valid_mask.sum().item()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100*correct/total:.2f}%'\n",
    "        })\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader) if len(dataloader) > 0 else 0\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device, label_encoder):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_answer_texts = []\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Evaluating\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Filter out unknown labels\n",
    "            valid_mask = labels != -1\n",
    "            if not valid_mask.any():\n",
    "                continue\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(pixel_values, input_ids, attention_mask)\n",
    "            loss = criterion(logits[valid_mask], labels[valid_mask])\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Get predictions\n",
    "            predictions = torch.argmax(logits[valid_mask], dim=-1)\n",
    "\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels[valid_mask].cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader) if len(dataloader) > 0 else 0\n",
    "    \n",
    "    if len(all_labels) > 0:\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "    else:\n",
    "        accuracy = 0\n",
    "        f1 = 0\n",
    "\n",
    "    return avg_loss, accuracy, f1, all_predictions, all_labels\n",
    "\n",
    "\n",
    "print(\"\\n‚úÖ Training and evaluation functions prepared\")\n",
    "\n",
    "## üèãÔ∏è 9. Start Training\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [], \n",
    "    'val_acc': [],\n",
    "    'val_f1': []\n",
    "}\n",
    "\n",
    "best_val_acc = 0\n",
    "best_model_path = 'best_vqa_model_vit_biobert.pth'\n",
    "\n",
    "print(\"\\nüèãÔ∏è Starting training...\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Training\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, train_loader, optimizer, scheduler, criterion, device, epoch\n",
    "    )\n",
    "\n",
    "    # Validation\n",
    "    val_loss, val_acc, val_f1, _, _ = evaluate(\n",
    "        model, val_loader, criterion, device, label_encoder\n",
    "    )\n",
    "\n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_f1'].append(val_f1)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\nüìä Epoch {epoch+1}/{NUM_EPOCHS} Results:\")\n",
    "    print(f\"   Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"   Val Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}% | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'val_f1': val_f1,\n",
    "            'label_encoder': label_encoder,\n",
    "            'history': history,\n",
    "            'model_config': {\n",
    "                'num_classes': num_classes,\n",
    "                'hidden_dim': 512,\n",
    "                'num_heads': 8,\n",
    "                'dropout': 0.2\n",
    "            }\n",
    "        }, best_model_path)\n",
    "        print(f\"   ‚úÖ Best model saved! (Val Acc: {val_acc*100:.2f}%)\")\n",
    "\n",
    "    print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüéâ Training completed!\")\n",
    "print(f\"‚úÖ Best validation accuracy: {best_val_acc*100:.2f}%\")\n",
    "\n",
    "## üß™ 10. Evaluate Model on Test Data\n",
    "print(\"\\nüß™ Evaluating model on test data...\")\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load(best_model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Evaluate on test data\n",
    "test_loss, test_acc, test_f1, test_predictions, test_labels = evaluate(\n",
    "    model, test_loader, criterion, device, label_encoder\n",
    ")\n",
    "\n",
    "print(f\"üìä Test Results:\")\n",
    "print(f\"   Test Loss: {test_loss:.4f}\")\n",
    "print(f\"   Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"   Test F1-Score: {test_f1:.4f}\")\n",
    "\n",
    "## üìà 11. Show Training Results\n",
    "def predict_answer(image_path, question, model, label_encoder, device):\n",
    "    \"\"\"\n",
    "    Function to predict answer for a given image and question\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading image: {e}\")\n",
    "        return \"Unable to load image\"\n",
    "    \n",
    "    # Process image\n",
    "    vision_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "    image_inputs = vision_processor(images=image, return_tensors='pt')\n",
    "    pixel_values = image_inputs['pixel_values'].to(device)\n",
    "    \n",
    "    # Process question\n",
    "    tokenizer = AutoTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
    "    text_inputs = tokenizer(\n",
    "        question,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = text_inputs['input_ids'].to(device)\n",
    "    attention_mask = text_inputs['attention_mask'].to(device)\n",
    "    \n",
    "    # Prediction\n",
    "    with torch.no_grad():\n",
    "        logits = model(pixel_values, input_ids, attention_mask)\n",
    "        prediction = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # Convert number to text answer\n",
    "    try:\n",
    "        answer_text = label_encoder.inverse_transform([prediction.cpu().item()])[0]\n",
    "    except:\n",
    "        answer_text = \"Unknown answer\"\n",
    "    \n",
    "    return answer_text\n",
    "\n",
    "# Test function on example from test data\n",
    "if len(dataset['test']) > 0:\n",
    "    example = dataset['test'][0]\n",
    "    image_path = example['image_path']\n",
    "    question = example['question']\n",
    "    \n",
    "    print(f\"\\nüîç Testing prediction function:\")\n",
    "    print(f\"   Question: {question}\")\n",
    "    print(f\"   Image path: {image_path}\")\n",
    "    \n",
    "    answer = predict_answer(image_path, question, model, label_encoder, device)\n",
    "    print(f\"   Predicted answer: {answer}\")\n",
    "    print(f\"   Actual answer: {example['answer']}\")\n",
    "\n",
    "## üíæ 12. Save Final Model\n",
    "final_model_path = 'final_vqa_model_vit_biobert.pth'\n",
    "torch.save({\n",
    "    'epoch': NUM_EPOCHS,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'test_accuracy': test_acc,\n",
    "    'test_f1': test_f1,\n",
    "    'label_encoder': label_encoder,\n",
    "    'history': history,\n",
    "    'model_config': {\n",
    "        'num_classes': num_classes,\n",
    "        'hidden_dim': 512,\n",
    "        'num_heads': 8,\n",
    "        'dropout': 0.2\n",
    "    },\n",
    "    'training_info': {\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'num_epochs': NUM_EPOCHS,\n",
    "        'best_val_acc': best_val_acc\n",
    "    }\n",
    "}, final_model_path)\n",
    "\n",
    "print(f\"\\nüíæ Models saved:\")\n",
    "print(f\"   - {best_model_path} (best model)\")\n",
    "print(f\"   - {final_model_path} (final model)\")\n",
    "print(f\"\\nüéØ Processing and training completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
