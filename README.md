This study investigates the efficacy of two distinct architectural approaches for Medical Visual Question Answering (Med-VQA), a task designed to provide clinical decision support by generating natural language answers from medical images. Using the VQA-RAD radiology dataset, which features X-rays, CT, and MRI scans, the research compares a baseline CNN-RNN model (ResNet18 and LSTM) against an advanced Transformer-based architecture (Vision Transformer and BioBERT).
